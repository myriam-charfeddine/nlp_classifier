{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\myria\\OneDrive\\Bureau\\tv_series_analyzer\\nlp_env\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "from nltk import sent_tokenize\n",
    "import nltk\n",
    "import torch\n",
    "from glob import glob\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'all'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package abc to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\abc.zip.\n",
      "[nltk_data]    | Downloading package alpino to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\alpino.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\averaged_perceptron_tagger.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_eng.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_ru.zip.\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\averaged_perceptron_tagger_rus.zip.\n",
      "[nltk_data]    | Downloading package basque_grammars to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\basque_grammars.zip.\n",
      "[nltk_data]    | Downloading package bcp47 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package biocreative_ppi to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\biocreative_ppi.zip.\n",
      "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\bllip_wsj_no_aux.zip.\n",
      "[nltk_data]    | Downloading package book_grammars to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\book_grammars.zip.\n",
      "[nltk_data]    | Downloading package brown to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown.zip.\n",
      "[nltk_data]    | Downloading package brown_tei to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\brown_tei.zip.\n",
      "[nltk_data]    | Downloading package cess_cat to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_cat.zip.\n",
      "[nltk_data]    | Downloading package cess_esp to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cess_esp.zip.\n",
      "[nltk_data]    | Downloading package chat80 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\chat80.zip.\n",
      "[nltk_data]    | Downloading package city_database to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\city_database.zip.\n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\cmudict.zip.\n",
      "[nltk_data]    | Downloading package comparative_sentences to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\comparative_sentences.zip.\n",
      "[nltk_data]    | Downloading package comtrans to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package conll2000 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2000.zip.\n",
      "[nltk_data]    | Downloading package conll2002 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\conll2002.zip.\n",
      "[nltk_data]    | Downloading package conll2007 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package crubadan to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\crubadan.zip.\n",
      "[nltk_data]    | Downloading package dependency_treebank to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dependency_treebank.zip.\n",
      "[nltk_data]    | Downloading package dolch to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\dolch.zip.\n",
      "[nltk_data]    | Downloading package europarl_raw to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\europarl_raw.zip.\n",
      "[nltk_data]    | Downloading package extended_omw to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package floresta to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\floresta.zip.\n",
      "[nltk_data]    | Downloading package framenet_v15 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v15.zip.\n",
      "[nltk_data]    | Downloading package framenet_v17 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\framenet_v17.zip.\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gazetteers.zip.\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\genesis.zip.\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\gutenberg.zip.\n",
      "[nltk_data]    | Downloading package ieer to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ieer.zip.\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\inaugural.zip.\n",
      "[nltk_data]    | Downloading package indian to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\indian.zip.\n",
      "[nltk_data]    | Downloading package jeita to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package kimmo to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\kimmo.zip.\n",
      "[nltk_data]    | Downloading package knbc to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package large_grammars to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\large_grammars.zip.\n",
      "[nltk_data]    | Downloading package lin_thesaurus to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\lin_thesaurus.zip.\n",
      "[nltk_data]    | Downloading package mac_morpho to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mac_morpho.zip.\n",
      "[nltk_data]    | Downloading package machado to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package masc_tagged to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker.zip.\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping chunkers\\maxent_ne_chunker_tab.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\maxent_treebank_pos_tagger.zip.\n",
      "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping\n",
      "[nltk_data]    |       taggers\\maxent_treebank_pos_tagger_tab.zip.\n",
      "[nltk_data]    | Downloading package moses_sample to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\moses_sample.zip.\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data]    | Downloading package mte_teip5 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\mte_teip5.zip.\n",
      "[nltk_data]    | Downloading package mwa_ppdb to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\mwa_ppdb.zip.\n",
      "[nltk_data]    | Downloading package names to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\names.zip.\n",
      "[nltk_data]    | Downloading package nombank.1.0 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nonbreaking_prefixes.zip.\n",
      "[nltk_data]    | Downloading package nps_chat to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\nps_chat.zip.\n",
      "[nltk_data]    | Downloading package omw to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package omw-1.4 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package opinion_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\opinion_lexicon.zip.\n",
      "[nltk_data]    | Downloading package panlex_swadesh to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package paradigms to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\paradigms.zip.\n",
      "[nltk_data]    | Downloading package pe08 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pe08.zip.\n",
      "[nltk_data]    | Downloading package perluniprops to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping misc\\perluniprops.zip.\n",
      "[nltk_data]    | Downloading package pil to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pil.zip.\n",
      "[nltk_data]    | Downloading package pl196x to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pl196x.zip.\n",
      "[nltk_data]    | Downloading package porter_test to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\porter_test.zip.\n",
      "[nltk_data]    | Downloading package ppattach to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ppattach.zip.\n",
      "[nltk_data]    | Downloading package problem_reports to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\problem_reports.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_1 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_1.zip.\n",
      "[nltk_data]    | Downloading package product_reviews_2 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\product_reviews_2.zip.\n",
      "[nltk_data]    | Downloading package propbank to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package pros_cons to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\pros_cons.zip.\n",
      "[nltk_data]    | Downloading package ptb to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ptb.zip.\n",
      "[nltk_data]    | Downloading package punkt to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt_tab to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data]    | Downloading package qc to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\qc.zip.\n",
      "[nltk_data]    | Downloading package reuters to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package rslp to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping stemmers\\rslp.zip.\n",
      "[nltk_data]    | Downloading package rte to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\rte.zip.\n",
      "[nltk_data]    | Downloading package sample_grammars to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\sample_grammars.zip.\n",
      "[nltk_data]    | Downloading package semcor to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package senseval to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\senseval.zip.\n",
      "[nltk_data]    | Downloading package sentence_polarity to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentence_polarity.zip.\n",
      "[nltk_data]    | Downloading package sentiwordnet to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sentiwordnet.zip.\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\shakespeare.zip.\n",
      "[nltk_data]    | Downloading package sinica_treebank to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\sinica_treebank.zip.\n",
      "[nltk_data]    | Downloading package smultron to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\smultron.zip.\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package spanish_grammars to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping grammars\\spanish_grammars.zip.\n",
      "[nltk_data]    | Downloading package state_union to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\state_union.zip.\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package subjectivity to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\subjectivity.zip.\n",
      "[nltk_data]    | Downloading package swadesh to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\swadesh.zip.\n",
      "[nltk_data]    | Downloading package switchboard to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\switchboard.zip.\n",
      "[nltk_data]    | Downloading package tagsets to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets.zip.\n",
      "[nltk_data]    | Downloading package tagsets_json to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping help\\tagsets_json.zip.\n",
      "[nltk_data]    | Downloading package timit to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\timit.zip.\n",
      "[nltk_data]    | Downloading package toolbox to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\toolbox.zip.\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\treebank.zip.\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\twitter_samples.zip.\n",
      "[nltk_data]    | Downloading package udhr to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr.zip.\n",
      "[nltk_data]    | Downloading package udhr2 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\udhr2.zip.\n",
      "[nltk_data]    | Downloading package unicode_samples to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\unicode_samples.zip.\n",
      "[nltk_data]    | Downloading package universal_tagset to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping taggers\\universal_tagset.zip.\n",
      "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package vader_lexicon to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package verbnet to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet.zip.\n",
      "[nltk_data]    | Downloading package verbnet3 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\verbnet3.zip.\n",
      "[nltk_data]    | Downloading package webtext to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\webtext.zip.\n",
      "[nltk_data]    | Downloading package wmt15_eval to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\wmt15_eval.zip.\n",
      "[nltk_data]    | Downloading package word2vec_sample to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping models\\word2vec_sample.zip.\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2021 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet2022 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet2022.zip.\n",
      "[nltk_data]    | Downloading package wordnet31 to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\wordnet_ic.zip.\n",
      "[nltk_data]    | Downloading package words to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\words.zip.\n",
      "[nltk_data]    | Downloading package ycoe to\n",
      "[nltk_data]    |     C:\\Users\\myria\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]    |   Unzipping corpora\\ycoe.zip.\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection all\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('all')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'facebook/bart-large-mnli'\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(device):\n",
    "    theme_classifier = pipeline(\n",
    "        'zero-shot-classification',\n",
    "        model = model_name,\n",
    "        device= device\n",
    "    )\n",
    "\n",
    "    return theme_classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\myria\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:148: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\myria\\.cache\\huggingface\\hub\\models--facebook--bart-large-mnli. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "theme_classifier = load_model(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_list = ['friendship', 'hope', 'sacrifice', 'battle', 'self development', 'betrayal', 'love', 'dialogue']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I punch him right in the face then left madly',\n",
       " 'labels': ['battle',\n",
       "  'self development',\n",
       "  'betrayal',\n",
       "  'sacrifice',\n",
       "  'love',\n",
       "  'hope',\n",
       "  'dialogue',\n",
       "  'friendship'],\n",
       " 'scores': [0.8379108905792236,\n",
       "  0.19707369804382324,\n",
       "  0.1469639241695404,\n",
       "  0.084993377327919,\n",
       "  0.017587676644325256,\n",
       "  0.003336874069646001,\n",
       "  0.0023824165109544992,\n",
       "  7.160699169617146e-05]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theme_classifier(\n",
    "    'I punch him right in the face then left madly',\n",
    "    theme_list,\n",
    "    multi_label = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_files = glob('../data/Subtitles/*.ass')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(data_files[0], 'r') as file:\n",
    "    lines = file.readlines()\n",
    "    lines = lines[27:]\n",
    "    lines = [','.join(line.replace('\\\\N', ' ').strip().split(',')[9:]) for line in lines ]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(data_files[0].split('-')[-1].split('.')[0].strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load subtitles and process text and episode number\n",
    "def load_subtitles(subtitles_files_path):\n",
    "    paths = glob(subtitles_files_path + '/*.ass')\n",
    "    \n",
    "    scripts = []\n",
    "    episodes_num = []\n",
    "\n",
    "    for path in paths:\n",
    "        with open(path, 'r', encoding=\"utf8\") as file:\n",
    "            lines = file.readlines()\n",
    "            lines = lines[27:]\n",
    "            lines = [','.join(line.replace('\\\\N', ' ').strip().split(',')[9:]) for line in lines ]\n",
    "\n",
    "        script = ' '.join(lines)\n",
    "        \n",
    "        episode_num = int(path.split('-')[-1].split('.')[0].strip())\n",
    "\n",
    "        scripts.append(script)\n",
    "        episodes_num.append(episode_num)\n",
    "\n",
    "    df = pd.DataFrame.from_dict({'Episode' : episodes_num, 'Script' : scripts})\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_subtitles('../data/Subtitles')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Episode</th>\n",
       "      <th>Script</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>211</td>\n",
       "      <td>Fly into the wavy and twisted sky, into your h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>212</td>\n",
       "      <td>Fly into the wavy and twisted sky, into your h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>213</td>\n",
       "      <td>Fly into the wavy and twisted sky, into your h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>214</td>\n",
       "      <td>Fly into the wavy and twisted sky, into your h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>215</td>\n",
       "      <td>Fly into the wavy and twisted sky, into your h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>216</td>\n",
       "      <td>Summoning Jutsu! Oh, long time no see. We don’...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>217</td>\n",
       "      <td>Gaara. Kankuro. Gaara. Primary Lotus! Damn it!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>218</td>\n",
       "      <td>To think the Leaf’s reinforcements will be you...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>219</td>\n",
       "      <td>Fly into the wavy and twisted sky, into your h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>220</td>\n",
       "      <td>So this is the Shukaku? It’s the first time I’...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Episode                                             Script\n",
       "208      211  Fly into the wavy and twisted sky, into your h...\n",
       "209      212  Fly into the wavy and twisted sky, into your h...\n",
       "210      213  Fly into the wavy and twisted sky, into your h...\n",
       "211      214  Fly into the wavy and twisted sky, into your h...\n",
       "212      215  Fly into the wavy and twisted sky, into your h...\n",
       "213      216  Summoning Jutsu! Oh, long time no see. We don’...\n",
       "214      217  Gaara. Kankuro. Gaara. Primary Lotus! Damn it!...\n",
       "215      218  To think the Leaf’s reinforcements will be you...\n",
       "216      219  Fly into the wavy and twisted sky, into your h...\n",
       "217      220  So this is the Shukaku? It’s the first time I’..."
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A long time ago, a powerful demon fox appeared with nine tails. With its powerful tails, it could smash mountains and create tidal waves. A band of Ninjas rose to defend their village from attack. We have to wait until the Fourth Hokage gets here! We can\\'t let it get any closer to our village! One great Ninja was able to imprison the monster, but died in the process. This Ninja was known as… the Fourth Hokage. Naruto! Why did you do such a thing?! You\\'re really gonna get it this time! I don\\'t care! You know your problem? You can\\'t do the things I do! Only I can do this! I\\'m better than all of you! Believe it! There\\'s a problem, sir! Lord Hokage! What is it? Did that Naruto do something again? Yes. He climbed onto the Mountainside Images… And he vandalized and graffitied all over them! Wait! Ha ha… Why should I? Hey, Naruto! How did you suddenly get here, lruka Sensei? The question is what are you doing here when you should be in class now? Now listen, Naruto. You failed the last graduation test and the one before that. This is no time to be goofing off, you fool! We will have a re-test on the Transformation Jutsu! Even those who already passed will take it! Whaaaat?! Sakura Haruno. Here I go… Transform! OK! I did it! Cha! Did you see that, Sasuke? Next, Sasuke Uchiha. Yes. O-OK. Next, Naruto Uzumaki. This is a real pain. And it\\'s all your fault. Like I care!! OK… Good luck, Naruto… Transform! How was it? I call it the \"Sexy Jutsu\"! You fool! Stop making idiotic spells! Darn… Darn… I won\\'t let you go home unless you clean that all up. I don\\'t care… There\\'s nobody home anyway. Naruto... What is it this time? What I meant was… If you clean up all that mess, I\\'ll buy you ramen tonight. Huh?! Yes! I-I will finish it no time! Enter: Naruto Uzumaki! Naruto. Why did you vandalize those faces? Don\\'t you know who the Hokage leaders are? Of course, I do! I know they earned the title Lord Hokage because they were the best Ninja of their time, right? Especially the Fourth Hokage was a hero who saved the village from the nine-tail demon fox. Then why did you do that? Because I\\'ll become a Hokage myself. And I\\'ll be the greatest Hokage of all time! So that everyone will finally learn to accept me! By the way, Sensei, I have a favor to ask. You want another bowl? Mmmm…No… Can I borrow that Leaf headband for a while? This? No no! This is worn only by those who have graduated from Ninja Academy. Tomorrow, you will… You\\'re so mean! So that\\'s why you took off your goggles… Humph... One more bowl please! We are now about to begin the graduation test. When your name is called, proceed to the next classroom. The test is on the Clone Jutsu. Oh no… Of all the…! That is my weakest Jutsu! But still… I will do it no matter what! Clone Jutsu! Disqualified! Iruka Sensei. His physical coordination and stamina are excellent. And he managed to come up with something. Isn\\'t that enough for him to pass? Mizuki Sensei... All the others created three or more clones. Naruto created just one. And it\\'s practically useless. I can\\'t give him a passing mark. I \\'m a Ninja now! You did well. That\\'s my son. Congratulations for your graduation. I\\'ll cook something good tonight! Look at that one. It\\'s that boy. I hear he\\'s the only one who failed. Serves him right. Imagine what would happen if he became a Ninja. Isn\\'t that the boy who is actually… Hey! We\\'re not supposed to talk about that. Iruka. We need to talk later. Yes, sir. Iruka Sensei isn\\'t trying to be mean to you. Then why only me? He wants you to become strong from the bottom of his heart. You both don\\'t have parents. But I really wanted to graduate. Heh... I guess I have no choice… I\\'ll let you in on a big secret. Secret? Iruka. What is it, Lord Hokage? I know how you feel. But… Naruto also grew up without knowing the love of his parents…like you. Let me go! My mom and dad are still out there fighting! Wake up, Iruka Sensei! What\\'s the matter? Come to Lord Hokage\\'s immediately! I heard that Naruto… stole the Scroll of Sealing. The Scroll of Sealing?! Let\\'s see… The first Jutsu is… Multi-Shadow Clone Jutsu? What?! Already a Jutsu I\\'m no good at? Lord Hokage! We can\\'t forgive him! This is not just a prank! The Scroll is a dangerous item that the First Hokage sealed! Depending on its use… It will be a major disaster if it is taken out of the village! Yes. Bring Naruto here at once! Yes, sir! Where did you go…Naruto? I will tell everyone in the village about this and eliminate Naruto… Then the Scroll of Sealing will be mine! Hey you, Naruto! You found me.. And I\\'ve only learned one Jutsu. He\\'s been practicing the Jutsu… until he\\'s become this exhausted and dirty…? Listen, listen! I\\'m gonna show you this amazing Jutsu! You\\'re gonna let me graduate if I can do it! Isn\\'t it true that I can graduate if I can do one of the Jutsu written here? Who told you that? Mizuki Sensei. He told me about this scroll, and this place… Mizuki did?! I\\'m impressed you found this place. I see now…how it is. Naruto, give me that scroll. Wait, wait… What\\'s going on here? Naruto! Never give him that scroll! It is a dangerous object that contains forbidden Ninja Jutsu. It was sealed. Mizuki used you in order to get it for himself! W-Wha--? Naruto, Iruka is only afraid of you holding that scroll! Huh? What are you saying, Mizuki! Don\\'t let him fool you, Naruto! I will tell you the truth. Idiot! Don\\'t do that! After an incident 12 years ago, a rule was created. A rule? That is, Naruto, a rule everybody but you knows. Except me?! \\t\\t\\t\\t\\tWhat is it? Stop it, Mizuki! The rule forbids anyone from revealing that you are actually the Demon Fox Spirit! Huh? You are actually the Demon, Nine-Tailed Fox Spirit, who killed Iruka\\'s parents and destroyed our village! Stop it! Everyone has been deceiving you ever since. Didn\\'t you find it strange? Why everyone hated you so much? No! No! No! No! No! Naruto… Nobody accepts you. That\\'s why Iruka hates you so much! Iruka... Naruto grew up without the love of parents. Everyone avoids him like the plague after what happened. That\\'s why he keeps misbehaving. It\\'s the only way for him to get any attention or acknowledgement. He pretends to be tough, but inside he is really hurting. Die, Naruto! Naruto! \\t\\t\\t\\t\\tGet down! Why…? Because you and I are the same. After my parents died, nobody paid attention to me or gave me any support. I wasn\\'t a good student in school. I was the class clown… because I wanted people to notice me. I couldn\\'t get noticed through excellence, so I kept doing stupid things. It was so hard. Isn\\'t that right, Naruto? You felt so lonely…right? And you suffered inside, right? I\\'m sorry, Naruto…. If I had been more responsible, maybe you wouldn\\'t have suffered so much. Don\\'t make me laugh! Iruka has always hated you, ever since you killed his parents! He\\'s just saying all that to get the Scroll of Sealing back! Naruto! Narutoooooo! He is not the type of kid who will change his mind. He will take revenge against our village using that scroll! Didn\\'t you see his eyes? Those are the eyes of a Demon Fox. No… Naruto…isn\\'t…like that at all! All I want is to kill Naruto and get the scroll. I\\'ll take care of you later! I-I won\\'t let you… Well, well. Mizuki has a big mouth! Naruto feels worse than he\\'s ever felt. He might unleash the power locked up inside him. The Scroll of Sealing is now with him. There\\'s a slight chance he might actually release the Nine-Tailed Fox Spirit sealed inside him! If that happens… I\\'ve found him! Naruto! Everything that Mizuki said was a lie! Give me that scroll, quick! Mizuki is after the scroll! It can\\'t be… Why is it, Naruto? How… did you know I wasn\\'t Iruka…? Because I\\'m Iruka. I see. What\\'s in it for you to protect the one who killed your family? I\\'m not gonna let a stupid idiot like you get that scroll! You\\'re the idiot. Naruto is the same as me. Same? Anyone can do whatever he wants once he has the scroll. There is no way that that monster… that Fox Spirit, won\\'t take advantage of the power of that scroll! You\\'re right… I guess it was true all along! See, Iruka Sensei never really cared for me at all! ...if he was the Demon Fox Spirit. But Naruto is different! I know that he is an exceptional student. He works very hard, and he\\'s single-minded and clumsy at the same time. No one accepts him, but he knows the meaning of human suffering. He is not the Demon Fox Spirit. He\\'s Naruto Uzumaki of the Village Hidden in the Leaves! You are so gullible. \\t\\t\\t\\t\\tlruka! I was gonna take you down later, but I have changed my mind. Die! I guess this is the end for me… Naruto?! You surprised me there, freak. If you ever lay a hand on Iruka Sensei, I\\'ll kill you! Shut up! I can take care of a kid like you with a single blow! Why don\\'t you try then? I\\'ll strike you back a thousand-fold! Let\\'s see you try! Show me what you can do, Demon Fox! Shadow Clone Jutsu! Naruto! You\\'ve… Those aren\\'t just images but actual clones! That\\'s an advanced Ninjutsu! What\\'s this…? What\\'s the matter? \\t\\t\\t\\t\\tC\\'mon! Weren\\'t you gonna get me with one blow? Here! In that case… I\\'ll come to you. I kinda got carried away. lruka Sensei, are you okay? Yeah. He\\'s really something. Maybe it is true. Maybe he will surpass all the Hokage leaders… Naruto, come over here. I\\'d like to give you something. Has anyone found Naruto yet? No. Darn, this is going to be bad… There\\'s no need to worry anymore. Lord Hokage! He\\'ll be back soon. Sensei, how much longer? OK, you may open your eyes now. Congratulations…on your graduation. In celebration, we\\'ll have ramen tonight! Iruka Sensei! That hurts! Naruto… I was going to lecture to you... that the road gets more difficult now that you\\'re a Ninja. But I guess I\\'ll just wait to tell you that until we get to the ramen stand… W-What do you want, you little shrimp? Quit following me! You\\'re smaller than me and you\\'re saying that you\\'re gonna become the Fifth Hokage? I don\\'t care if you are the 3rd Hokage\\'s grandson or not. It\\'s not that easy to be a Hokage! If you want it that bad, you\\'re gonna have to beat me first! Next episode:  \"My Name Is Konohamaru!\" Watch my outstanding performance!'"
      ]
     },
     "execution_count": 154,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "script = df.iloc[0]['Script'] # .iloc make it possible to access data using index, in the example we access the column Script of the first row\n",
    "script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "script_sentences = sent_tokenize(script)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "#batch sentences\n",
    "sentences_batch_size = 20\n",
    "scripts_batches = []\n",
    "\n",
    "for i in range(0, len(script_sentences), sentences_batch_size):\n",
    "    sent = ' '.join(script_sentences[i : i+sentences_batch_size])\n",
    "    scripts_batches.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"A long time ago, a powerful demon fox appeared with nine tails. With its powerful tails, it could smash mountains and create tidal waves. A band of Ninjas rose to defend their village from attack. We have to wait until the Fourth Hokage gets here! We can't let it get any closer to our village! One great Ninja was able to imprison the monster, but died in the process. This Ninja was known as… the Fourth Hokage. Naruto! Why did you do such a thing?! You're really gonna get it this time! I don't care! You know your problem? You can't do the things I do! Only I can do this! I'm better than all of you! Believe it! There's a problem, sir! Lord Hokage! What is it? Did that Naruto do something again?\",\n",
       " 'Yes. He climbed onto the Mountainside Images… And he vandalized and graffitied all over them! Wait! Ha ha… Why should I? Hey, Naruto! How did you suddenly get here, lruka Sensei? The question is what are you doing here when you should be in class now? Now listen, Naruto. You failed the last graduation test and the one before that. This is no time to be goofing off, you fool! We will have a re-test on the Transformation Jutsu! Even those who already passed will take it! Whaaaat?! Sakura Haruno. Here I go… Transform! OK! I did it! Cha! Did you see that, Sasuke? Next, Sasuke Uchiha.',\n",
       " 'Yes. O-OK. Next, Naruto Uzumaki. This is a real pain. And it\\'s all your fault. Like I care!! OK… Good luck, Naruto… Transform! How was it? I call it the \"Sexy Jutsu\"! You fool! Stop making idiotic spells! Darn… Darn… I won\\'t let you go home unless you clean that all up. I don\\'t care… There\\'s nobody home anyway. Naruto... What is it this time? What I meant was… If you clean up all that mess, I\\'ll buy you ramen tonight. Huh?! Yes! I-I will finish it no time! Enter: Naruto Uzumaki! Naruto. Why did you vandalize those faces?']"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scripts_batches[0:3] #each bacth has 20 sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_classification = theme_classifier(\n",
    "    scripts_batches[0:3],\n",
    "    theme_list,\n",
    "    multi_label=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'sequence': \"A long time ago, a powerful demon fox appeared with nine tails. With its powerful tails, it could smash mountains and create tidal waves. A band of Ninjas rose to defend their village from attack. We have to wait until the Fourth Hokage gets here! We can't let it get any closer to our village! One great Ninja was able to imprison the monster, but died in the process. This Ninja was known as… the Fourth Hokage. Naruto! Why did you do such a thing?! You're really gonna get it this time! I don't care! You know your problem? You can't do the things I do! Only I can do this! I'm better than all of you! Believe it! There's a problem, sir! Lord Hokage! What is it? Did that Naruto do something again?\",\n",
       "  'labels': ['dialogue',\n",
       "   'betrayal',\n",
       "   'battle',\n",
       "   'sacrifice',\n",
       "   'self development',\n",
       "   'hope',\n",
       "   'friendship',\n",
       "   'love'],\n",
       "  'scores': [0.9790682792663574,\n",
       "   0.9497532248497009,\n",
       "   0.8569772839546204,\n",
       "   0.7666406035423279,\n",
       "   0.7487459778785706,\n",
       "   0.19316503405570984,\n",
       "   0.06438494473695755,\n",
       "   0.04339267313480377]},\n",
       " {'sequence': 'Yes. He climbed onto the Mountainside Images… And he vandalized and graffitied all over them! Wait! Ha ha… Why should I? Hey, Naruto! How did you suddenly get here, lruka Sensei? The question is what are you doing here when you should be in class now? Now listen, Naruto. You failed the last graduation test and the one before that. This is no time to be goofing off, you fool! We will have a re-test on the Transformation Jutsu! Even those who already passed will take it! Whaaaat?! Sakura Haruno. Here I go… Transform! OK! I did it! Cha! Did you see that, Sasuke? Next, Sasuke Uchiha.',\n",
       "  'labels': ['dialogue',\n",
       "   'self development',\n",
       "   'betrayal',\n",
       "   'battle',\n",
       "   'sacrifice',\n",
       "   'hope',\n",
       "   'friendship',\n",
       "   'love'],\n",
       "  'scores': [0.9152395725250244,\n",
       "   0.8458867073059082,\n",
       "   0.6924763917922974,\n",
       "   0.6468244194984436,\n",
       "   0.5885695219039917,\n",
       "   0.1360895335674286,\n",
       "   0.05948065593838692,\n",
       "   0.01833723671734333]},\n",
       " {'sequence': 'Yes. O-OK. Next, Naruto Uzumaki. This is a real pain. And it\\'s all your fault. Like I care!! OK… Good luck, Naruto… Transform! How was it? I call it the \"Sexy Jutsu\"! You fool! Stop making idiotic spells! Darn… Darn… I won\\'t let you go home unless you clean that all up. I don\\'t care… There\\'s nobody home anyway. Naruto... What is it this time? What I meant was… If you clean up all that mess, I\\'ll buy you ramen tonight. Huh?! Yes! I-I will finish it no time! Enter: Naruto Uzumaki! Naruto. Why did you vandalize those faces?',\n",
       "  'labels': ['dialogue',\n",
       "   'self development',\n",
       "   'betrayal',\n",
       "   'battle',\n",
       "   'sacrifice',\n",
       "   'hope',\n",
       "   'friendship',\n",
       "   'love'],\n",
       "  'scores': [0.9280455708503723,\n",
       "   0.7924256920814514,\n",
       "   0.6690558791160583,\n",
       "   0.6658564209938049,\n",
       "   0.351496160030365,\n",
       "   0.2414838969707489,\n",
       "   0.14436203241348267,\n",
       "   0.050931643694639206]}]"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_classification\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = {}\n",
    "for output in output_classification:\n",
    "    for label, score in zip(output['labels'], output['scores']):\n",
    "        if label not in themes:\n",
    "            themes[label]=[]\n",
    "        \n",
    "        themes[label].append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialogue': [0.9790682792663574, 0.9152395725250244, 0.9280455708503723],\n",
       " 'betrayal': [0.9497532248497009, 0.6924763917922974, 0.6690558791160583],\n",
       " 'battle': [0.8569772839546204, 0.6468244194984436, 0.6658564209938049],\n",
       " 'sacrifice': [0.7666406035423279, 0.5885695219039917, 0.351496160030365],\n",
       " 'self development': [0.7487459778785706,\n",
       "  0.8458867073059082,\n",
       "  0.7924256920814514],\n",
       " 'hope': [0.19316503405570984, 0.1360895335674286, 0.2414838969707489],\n",
       " 'friendship': [0.06438494473695755, 0.05948065593838692, 0.14436203241348267],\n",
       " 'love': [0.04339267313480377, 0.01833723671734333, 0.050931643694639206]}"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "themes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_theme_classification(script):\n",
    "  #script tokenization\n",
    "  script_sentences = sent_tokenize(script)\n",
    "  \n",
    "  #script sentences batching\n",
    "  sentences_batch_size = 20\n",
    "  scripts_batches = []\n",
    "  for i in range(0, len(script_sentences), sentences_batch_size):\n",
    "    sent = ' '.join(script_sentences[i : i+sentences_batch_size])\n",
    "    scripts_batches.append(sent)\n",
    "\n",
    "  #script sentences theme classification\n",
    "  theme_classification_output = theme_classifier(\n",
    "    scripts_batches,\n",
    "    theme_list,\n",
    "    multi_label=True\n",
    "  )\n",
    "  \n",
    "  #wrangling output : clean, transform data into a structured format\n",
    "  themes = {}\n",
    "  for output in theme_classification_output:\n",
    "    for label, score in zip(output['labels'], output['scores']):\n",
    "        if label not in themes:\n",
    "            themes[label]=[]\n",
    "        themes[label].append(score)\n",
    "  \n",
    "  themes = {key: np.mean(values) for key, values in themes.items()}\n",
    "  \n",
    "  return themes\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "themes = get_theme_classification(df.iloc[5]['Script'])\n",
    "themes = {key: np.mean(values) for key, values in themes.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[214], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m themes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msorted\u001b[39m(\u001b[43mthemes\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m(), key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[38;5;241m1\u001b[39m], reverse\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "themes = sorted(themes.items(), key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dialogue</th>\n",
       "      <th>self development</th>\n",
       "      <th>sacrifice</th>\n",
       "      <th>battle</th>\n",
       "      <th>betrayal</th>\n",
       "      <th>hope</th>\n",
       "      <th>friendship</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.8992</td>\n",
       "      <td>0.705038</td>\n",
       "      <td>0.696785</td>\n",
       "      <td>0.631374</td>\n",
       "      <td>0.404202</td>\n",
       "      <td>0.38491</td>\n",
       "      <td>0.352712</td>\n",
       "      <td>0.185532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   dialogue  self development  sacrifice    battle  betrayal     hope  \\\n",
       "0    0.8992          0.705038   0.696785  0.631374  0.404202  0.38491   \n",
       "\n",
       "   friendship      love  \n",
       "0    0.352712  0.185532  "
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_themess= pd.DataFrame([dict(themes)])\n",
    "df_themess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dialogue': 0.8992000073194504,\n",
       " 'self development': 0.705038278674086,\n",
       " 'sacrifice': 0.6967847421765327,\n",
       " 'battle': 0.6313737630844116,\n",
       " 'betrayal': 0.40420166961848736,\n",
       " 'hope': 0.38490963509927195,\n",
       " 'friendship': 0.3527124511698882,\n",
       " 'love': 0.18553190561942756}"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop('dialogue', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "theme_output = df.drop(['Episode', 'Script'], axis=1).sum().reset_index()\n",
    "theme_output.columns = ['Theme', 'Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(data= theme_output, x='Theme', y='Score')\n",
    "plt.xticks(rotation=45)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
